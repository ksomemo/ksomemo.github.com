だいぶゆるくまとめている

## 動機
以下の本を読んだり、書籍以外を読んだのでまとめて理解を深める

- キーポイント線形代数
    - 行列やベクトルについては別の本が必要
- ITエンジニアのための機械学習理論入門
    - 数式ありでPythonコード有り
    - ただし、NNやるならゼロから本の方がよさそう
- データマイニングの基礎
    - 数式多め詳しい
    - 情報量やエントロピー(KL, JS)
- Python機械学習
    - 数式はそんなに出てこないが、説明が丁寧
    - 機械学習の学習中に行う微分や行列系
- [ベイズ推定とグラフィカルモデル：コンピュータビジョン基礎1](https://www.udemy.com/computervision/learn/v4/overview)
    - ベイズの定理だけでなく、ベイズ推定まで学べる(特に周辺化重要)
    - ベイズ推定で使う事前分布と便利な共役分布について
    - 基本的な確率分布から複雑な確率分布を追加始める理由
        - 途中でEMアルゴリズム(K-Means/混合正規分布)
        - ラグランジュの未定乗数法(SVM/混合正規分布)
    - まだ途中

線形代数・行列の導出と意味を忘れ、機械学習のなんとか関数のブレにいつも悩まされるので、ここだけ見れば思い返せるようにする。また、他の書籍でわからなかったことも混じっている。

## 感想
- キーポイント線形代数はそれほど機械学習のための行列関連には直接関係なさそう
- ただし、直接関係ある公式などの理解を深めるための知識となる気がする

## 正方行列
行と列が等しい行列 N*N, not N*M (N≠M)

## 三角行列
正方行列に関すること

### 上三角行列
行列Xの成分を $X_{ij}$ としたとき $i > j$ が0である行列

### 下三角行列
行列Xの成分を $X_{ij}$ としたとき $i < j$ が0である行列

## 対角行列
上三角行列かつ下三角行列、つまり
行列Xの成分を $X_{ij}$ としたとき $i ≠ j$ が0である行列

正方行列 = 上三角行列 + 下三角行列 - 対角行列

## 単位行列
- 対角行列の対角成分がすべて1
- 他の行列をかけてもその行列の成分が変わらない

## trace
正方行列の対角成分の和

## 行列式
サラスの方法を使う（3次まで）

```
ax + by = p
cx + dy = q
```

の連立方程式,つまり

```
a b * x = p
c d   y   q
```

より、xとyの分母 ad - bc のこと

### 性質
1. 転置しても変わらない
2. ２つの行または列を入れ替えると、-1倍
3. 行が等しい、または列が等しいと、0
4. ある行または列をk倍すると、k倍
5. ある行または列の要素が２つの数の和になっていると、分解できる(和になっていない行または列はそのまま変化しない)
6. 別の行のk倍を加えても元の結果と変わらない
    - 5によるもとの行列式と別の行列式に分解
    - 4によるk倍行列式への変換
    - それによる3の出現で消える
7. 上三角行列の対角要素の積と等しい
    - 性質というより、単純に0になる要素の積を無視するとそうなる 
    - 行列式の行列部分を上三角行列にするとき、性質6を利用する

### クラメルの公式
公式より下記のほうが重要だと思う

- 3元1次連立方程式による解の導出時に現れる、3次の行列式の2次の行列式への分解
    - 逆行列の導出のときに使う
    - 導出途中、性質を使うと簡単に変換できる

#### 置換
- 符号
- あんまり重要度が分かっていない

## 逆行列
- 行列における逆数は？というところから始まる
- よって行列の積が出てくる
- 単位行列だけでなく、なぜかゼロ行列も出てくる

### 特異行列
- 逆行列が存在しない正方行列
- 行列式が0

### 余因子展開
- クラメルの公式のときに出てきた分解を使う
- 余因子を導入
- 余因子を使った行列式の余因子展開

#### 余因子分解とサラスの方法
行列の次数？を下げることができるので、4次を3次にしてサラスの方法を使うことで行列式を求めることが出来る

### 行列式の積
- AとBの行列の積の行列式から|A||B|の導出
    - ここでも性質を使う
    - 置換による行列式の定義を使う

### ようやく逆行列の公式
- 逆行列の定義 AA-1=Eから求める
- ここでクラメルの公式が活躍
- 余因子の別の書き方が出てくる

## ベクトル空間
## 線形変換

## 線形変換と定数倍と固有値分解
行列Aでゼロでないベクトルxを変換した時、変換後にベクトルxの定数倍になっている時、xを固有ベクトル、定数を固有値という。定数倍にならないベクトルが多い。


```math
Ax = \lambda x \\
(A - \lambda I)x = 0 \\
\
\left[\begin{matrix}
a_{11}-\lambda & a_{12} \\
a_{21}         & a_{22}-\lambda
\end{matrix}\right]
\left[\begin{matrix}
x_{1} \\
x_{2}
\end{matrix}\right]
= 0
```

### 連立方程式と行列式 = 0の意味
TODO

- 固有値を求める
- 固有値を代入して固有ベクトルを求める

入力行列の分散共分散行列 = 固有ベクトル * 固有値対角行列 * 固有ベクトル(転置)
変換後行列 = 入力行列 * 固有ベクトル

変換を

## 随伴行列
随伴行列は、行列の転置をしたあと、各成分の虚部の符号反転をしたもの。

## エルミート行列とユニタリ行列と対角化
エルミート行列は行列と随伴行列が同じであること。 行列の成分が実数のみで対称行列であればエルミート行列である。 シャルル・エルミート（Charles Hermite)。

ユニタリ行列とは、ある行列成分に虚部があるとき、行列と随伴行列の積が単位行列になる行列のことである。つまり、この時の随伴行列は逆行列でもある。また成分が実数のみである場合と比較すると、直交行列の拡張となる。

エルミート行列に対するユニタリ行列での対角化では、得られる対角行列の成分がすべて実数となるためである。エルミート行列を利用する理由はこれである（共分散分析が実数のみの対角行列であるため）。

TODO:対角化

### 標準化とジョルダン標準化

## その他

### 直交行列
逆行列をかけると単位行列になる行列

## 総和と平均
1. $\sum_{i=0}^n y_i ...①$ 
2. $① / n = \bar{y}$
3. $① = \bar{y}n$

### 例：単回帰分析の係数と切片

- 最小二乗法より、 $\sum{(y - \hat{y})^2} = 0$となるときのa, bを求める
- bは関係なく、aを求める時に使う
    - 連立方程式でbを消す
    - 解を共分散に変形する

参考: http://www.sist.ac.jp/~kanakubo/research/statistic/tankaiki.html

## 総和と微分
- 内側の式を微分してよいことが多い
- 行列に対して偏微分を用いて係数を求める際によく使う
- indexと関係ない場合はN倍すればよく、関係あるならばそのままにしておく

## 内積
### 意味
数学ガール秘密のノートに書いてあった

- 直角三角形における斜辺/底辺
- 単位円での角度の拡張

### 数式の解釈
- ベクトルで考えた時、n要素 * n要素の転置　= スカラー
- 各ベクトルの添字が同じものを掛けあわせて、足し込む
- Σ(a_i * b_i) -> A*B^T と書ける(式変形を理解しておくと、本の理解も進む)
- 行列の場合も同じ、nm * mn -> nn行列
    - ベクトルの場合 1m * m1 -> 11行列 -> スカラー
- 行列の外側と内側の次元数を一致させることに慣れれば、最終的な出力の形がわかる

#### 上記の解説
- http://d.hatena.ne.jp/rikunora/20130614/p1
- 大きさ同士の積を面積を求めると解釈するところから始まる
    - 長いベクトルの大きさと短いベクトル(長いベクトルの影)
    - 片方のベクトルを90度回転して(sin)長方形とする
    - 成分から面積を求める

すごく理解できたのと、幾何と数値を行き来する感覚がわかった。

内積の展開式=0=目的変数の分類の基準になるし、2次元平面で考えれば直線の上下での分類になる。
また0になるということは内積で考えると90°なので法線ベクトルという説明が出てくるのも納得できる。分類する直線に対して法線を引けるように重みを調整するとも考えられる。

## 分散共分散行列
$\frac{1}{N}XX^{ \mathrm{ T } }$

各変数の組合せすべての共分散と対角要素に変数の分散を持つ行列

- 対角行列であるとき、変数同士は独立
- 対角行列でないとき、回転した座標軸において変数同士は独立(に近い)

## 転置行列
### 用途
- 線形回帰の係数を求めるときに使った

```math
(AB)^{ \mathrm{ T } } = B^{ \mathrm{ T } }A^{ \mathrm{ T } }
```

```math
\begin{align}
w^{ \mathrm{ T } }\Phi^{ \mathrm{ T } }\Phi - \boldsymbol{ t }^{ \mathrm{ T } }\Phi &= 0 \\
(w^{ \mathrm{ T } }\Phi^{ \mathrm{ T } })\Phi &= \boldsymbol{ t }^{ \mathrm{ T } }\Phi \\

\Phi^{ \mathrm{ T } }(w^{ \mathrm{ T } }\Phi^{ \mathrm{ T } })^{ \mathrm{ T } } &= \Phi^{ \mathrm{ T } }\boldsymbol{ t } \\
左辺 &= \Phi^{ \mathrm{ T } }(\Phi w) \\

w &= (\Phi^{ \mathrm{ T } }\Phi)^{-1} \Phi^{ \mathrm{ T } }\boldsymbol{ t }
\end{align}
```
行列は可換じゃないので、部分ごとに行う?

### 手順と使った性質
- 両辺を転置
    - 積の転置は積の左右を入れ替えた転置の積を与える
        - [wikipedia:転置行列](https://ja.wikipedia.org/wiki/%E8%BB%A2%E7%BD%AE%E8%A1%8C%E5%88%97)
- 左から(Φの転置行列とΦ) の逆行列をかけて完了
    - 本では、上記が逆行列を持つことの確認をしている(ヘッセ行列)

## 指数
累乗の積は、指数部分を和にまとめられる

## 対数
- 対数の真数部分の積は、対数の和に分解できる
- 独立事象の確率の積を計算する際によく使う
    - 確率の積を何度も計算すると、計算機上ではオーバーフローする
    - 対数をとって和にすることで防ぐ

### odds/logit/logistic関数
[今更ながらoddsとlogitとlogistic関数](http://qiita.com/ksomemo/items/a65e0d1f17a9c7ef1a07) にまとめた

### 指数型分布族
- 読んだ本には出てきてないが、のちのちためになりそうなので読んだ
- ある要素で構成されるなら指数型分布となるらしい
- 正規分布やベルヌーイ分布など

#### exp(log(p)) = p: 底はe
- ベルヌーイ分布を出すときに使った
- 結果を知ってからだと当たり前だが、思いつかないということは指数対数の根本が危うい...
- 見出しの式の左辺の対数をとる
- log(exp ^ log(p)) = log(p) * log(e) = log(p) = 見出し右辺の対数

## 微分
### ベクトルの微分
- ベクトルのある要素i番目を例にあげて偏微分することが多い
- i番目以外は定数になるためi番目のみを考える
- その結果は要素i番目以外でも同じ形をする
- それぞれの要素を微分し、ベクトルにまとめなおす

### 2階微分とconvex
- 2階微分が正のとき、下に凸なので微分=0で最小化の最適解となる
- 1階微分=0だけでは局所解である可能性があるため、最適解とは限らない

### 正定値、半正定値とその意味
TODO

### ヘッセ行列
- 行列関数の2階微分
- 正定値行列、固有値が全て非負
- https://detail.chiebukuro.yahoo.co.jp/qa/question_detail/q11149976641 が親切だった

## テーラー展開
TODO

## ラプラス近似
TODO

## 期待値
```math
\begin{align}
E[X] &= \displaystyle \sum_{i=1}^{n} x_i P(X=x_i) \\
E[X] &= \displaystyle \int_ {-\infty}^{\infty}x P(x) dx \\
E[aX]  &= aE[X] \\
E[X+a] &= E[X] + a \\
E[X+Y] &= E[X] + E[Y]
\end{align}
```

## 分散
```math
\begin{align}
V[X] &= E[(X-E(X))^{2}] \\
&= \sum_{i=1}^{n} {(x_i - E[X] )^2 P(X=x_i)} \\
&= \sum_{i=1}^{n} {(x_i^{2}P(X=x_i) - 2x_iE[X]P(X=x_i) + E[X]^2P(X=x_i))} \\
&= \sum_{i=1}^{n} {x_i^{2}P(X=x_i)}
   -2E[X]\sum_{i=1}^{n} {x_iP(X=x_i)} 
   + (E[X])^2 \sum_{i=1}^{n} {P(X=x_i)} \\
&= E[X^{2}] -2(E[X])^2 + (E[X])^2 \\
&= E[X^{2}] - (E[X])^2
\end{align}
```

```math
\begin{align}
V[aX]  &=& a^2 V[X] \\
V[X+a] &=& V[X] 
\end{align}
```

忘れるのでメモする

## 記号

## 集合

## 論理

## 数列

## 級数
- 多項式の列を１つの項で近似？

### 時系列分析での例
ホワイトノイズやARで出てくる

- 上記の差分方程式に存在するlagを適用した差分項を除外する
- 上記は時系列項(確率変数)の式とするために行う

## 差分方程式

## 微分方程式

## 特性方程式

## 順列

## 組み合わせ

### 二項定理

### 多項定理

## 極限

## 積分

### 重積分

## ガンマ関数
確率分布

## ベータ関数
確率分布

## テイラー展開
- ベイジアン

## フーリエ変換
### フーリエ級数
http://www.ic.is.tohoku.ac.jp/~swk/lecture/yaruodsp/toc.html

## ラプラス近似
- ベイジアン

## ラグランジュの未定乗数法
ある項を条件付けないと、解が複数出てくるときによく使われている。
B=1であるとしたとき、λ(B-1)と差し引き0になるようにしたあとに係数をかける。
詳細はまだわかってない…

- SVM
- ベイジアン

## 三角関数

## 複素数

## 線形代数

### ベクトル

### 行列
#### フロベニウスノルム
ある行列と回転行列をかけた行列の差分についてL2ノルムをとると、traceを利用した形になる

### 行列分解
https://ja.wikipedia.org/wiki/%E8%A1%8C%E5%88%97%E3%81%AE%E5%88%86%E8%A7%A3

#### 対角化
#### 正定値
- 正定値行列
- 半正定値行列
- 半正定値対称行列

#### 固有値分解
説明済み

#### 線形独立
TODO：わかるけど説明できない

#### 小行列式
ある行列からr個の行と列を取り出して作った行列式(r次小行列式)

#### rank
いろいろな定義があるが、意味は同じらしい。

1. 行数。ただし、ガウスの消去法適用終了時に、成分のうち１つは0である行(0ベクトルでない行)の数。
2. 線形独立な列ベクトルの最大数。
3. 小行列式が0でない最大の次数r。ガウスの消去法を適用した上で0ベクトルが含まれる時、対角成分の積が0になる説明がわかりやすい
    - ガウスの消去法適用時の行列が特異値分解する前提の行列に似ている
4. 2の行ベクトル版

#### 特異値分解
正方行列でなくてもよい(主成分分析における特異値分解では、分散共分散行列の正方行列なので、理解が進んでいない)

TODO:いかを読む

- ttp://qiita.com/horiem/items/71380db4b659fb9307b4
- https://abicky.net/2010/09/05/131612/

#### QR分解
正方行列 = 直交行列 * 上三角行列

### 行列変換?投影
#### 剛体変換
Euclidean

平行移動＋回転

#### 相似変換
Similarity

剛体変換＋拡大縮小(奥行き移動)より相似

#### affine変換
相似＋回転ではなく傾き

#### 射影変換
homography

TODO: affineとの違いをどう説明するか

## 統計学

## 距離

## 数式慣れ
数式 to numpy code

- sum
- product
- dot
- argmin/max
- min/max
- log
- exp
- ndarray.T
- etc.



